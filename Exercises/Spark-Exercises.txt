
# Running spark on Hortonworks sandbox - 

Start your Hortonworks virtual machine - 

# Before using Spark using Hortonworks, we need to make some configuration changes using Ambari, for this you need to login as 
  admin, which is explained as below

# First of all you need to set admin password for Ambari, to do this you need to login to virtual machine via ssh using Putty(windows) or terminal (linux or Mac)

ssh maria_dev@127.0.0.1 -p 2222
(password is maria_dev)

Once logged in, you have to login as superuser (root)

# su root
(the default password is hadoop)
Note: you will be asked to reset the default password immediately
For the sake of easeness set the password as bigdata@roma (and remember this)

Once logged in as root, you will be able to reset the ambari admin password using,

# ambari-admin-password-reset 
(At this point youâ€™ll be prompted to enter your password for the Ambari admin user)
Note: set the password as bigdata@roma for easeness and uniformity

Once the reset is done, you have to restart the ambari agent for your changes to take effect. You can do this using,
# ambari-agent restart


Go to ambari - 
http://localhost:8080/
Login with admin with the new password

Hover over on the Services tab on top right and click on Spark

Click to Configs tabs and

Change the log level of Spark, for that

Click on Advanced Log4j-properties and change
log4j.rootCategory=INFO.console to log4j.rootCategory=ERROR.console

Hit save on top right and add a comment "Changed log level of Spark to ERROR" (you can give any comment)

For changes to take effect, you have to restart all services, so click on restart button on top right and wait for the process to complete

Do the same for Spark 2 service

# Next we have to ensure that we have movielens dataset inn our folder, /user/maria_dev 
a folder ml-100k with 2 files u.data and u.item,
Go to files view and check if this data is present are not, else download from internet and upload on to HDFS using Files view

or

you can login as maria_dev using putty, execute following commands
# mkdir ml-100k
# cd ml-100k
# wget http://media.sundog-soft.com/hadoop/ml-100k/u.item
# wget http://media.sundog-soft.com/hadoop/ml-100k/u.data

# cd ..
you should be in maria_dev directory now

# pwd
should give you /user/maria_dev

Get the scripts for running Spark on HDP cluster
# wget http://media.sundog-soft.com/hadoop/Spark.zip

Unzip the downloaded Spark.zip file

# unzip Spark.zip

to get actual scripts

The file that we want to execute is LowestRatedMovieSpark.py

To preview use cat command

# cat LowestRatedMovieSpark.py

To run this use following command spark-submit which sets up all the Spark related services and runs your script on the cluster,

# spark-submit LowestRatedMovieSpark.py

Note: spark-submit can take a bunch of parameters as well, like which cluster you want to run on or how much memory you want to allocate to each cluster etc.., Unless you are hadoop administrator it is not usually important



