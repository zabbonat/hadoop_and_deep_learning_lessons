Sample Map reduce exercises for practice - 


For movie lens data set - 

=> List of fields available: USER ID, MOVIE ID, RATING, TIMESTAMP

=> Ratings counter - Key: MOVIE ID, Value: RATING

=> Average rating per user - Key: USER ID, Value: RATING

=> Average rating per movie - Key: MOVIE ID, Value: RATING

=> No of movies each user watched - Key: USER ID, Value: MOVIE ID



Others - 

=> Count Friends by Age 

=> Temperature extremes (Minimum and Maximum) by Location

=> Word frequency counter

=> Word frequency counter using regular expressions

=> Sorting the frequency counter results using chained Map Reduce jobs

=> Customer orders count (based on individual customer)

=> Most popular movie

=> Most popular super hero

=> Similar movies (recommendation engine based on cosine similarity) 


# Running map reduce jobs on Horton works cluster

# First of all we need to install required softwares for Map reduce

# To install softwares on the Hortonworks sandbox we need to login as superuser by using command below
# su root 
(Initial password is hadoop and you are required to change it immediately)

# If you have installated ***HDP 2.6.5*** (Latest sandbox version) -

# Install PIP -
  Utility for installing Python packages
  yum install python-pip
  
# Install MR job
  pip install mrjob==0.5.11
  
# Install Nano text editor
  yum install nano
  
# Download data files and script
  wget http://media.sundog-soft.com/hadoop/ml-100k/u.data
  wget http://media.sundog-soft.com/hadoop/RatingsBreakdown.py
  
# If you have installated ***HDP 2.5*** (Old sandbox version) -

# Install PIP -
  cd /etc/yum.repos.d
  cp sandbox.repo /tmp
  rm sandbox.repo
  cd /home/maria_dev
  yum install python-pip
  
# Install MR job - 
  pip install google-api-python-client==1.6.4
  pip install mrjob==0.5.11
  
# Install Nano text editor
  yum install nano
   
# Download data files and script
  wget http://media.sundog-soft.com/hadoop/ml-100k/u.data - this is our ratings data file from our movie lens dataset
  wget http://media.sundog-soft.com/hadoop/RatingsBreakdown.py - this is the map reduce script 
  
  
 # Using nano
   nano file_name.py  - to open a file using nano editor
   ctrl + x - to come out of nano editor
   
  # Running with mrjob
  
  # Run locally - 
     python RatingsBreakdown.py u.data
     
  # Run with Hadoop - 
     python MostPopularMovie.py -r hadoop 
     --hadoop-streaming-jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar u.data
  
  # Run with Amazon EMR(Elastic map reduce) - 
  
  Note: For running a MapReduce(MR) job on Amazon EMR you first need an account on aws.amazon.com. 
        Once you give the card details, (you will be billed per usage and the cost of usage is also very less)
        you will be able to access the security keys (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
        to access map reduce on your local machine
  
  To run on a single EMR node: 
      > python MostPopularMovie.py -r emr --items=ml-100k/u.item   
          ml-100k/u.data
          
   To run on 4 EMR nodes: 
       > python MostPopularMovie.py -r emr --num-ec2-instances=4 --  
           items=ml-100k/u.item ml-100k/u.data

    Note: For the above commands to work we should be setting    
         AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY 
         prior to execution of the python script. 
         
         For windows: you need to create an environmental variables for these 2
         For Linux and Mac: you can just do export AWS_ACCESS_KEY_ID=$key_value$

  
  
  
 






