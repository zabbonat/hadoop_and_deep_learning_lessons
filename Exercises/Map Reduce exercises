Sample Map reduce exercises for practice - 


For movie lens data set - 

=> List of fields available: USER ID, MOVIE ID, RATING, TIMESTAMP

=> Ratings counter - Key: MOVIE ID, Value: RATING

=> Average rating per user - Key: USER ID, Value: RATING

=> Average rating per movie - Key: MOVIE ID, Value: RATING

=> No of movies each user watched - Key: USER ID, Value: MOVIE ID



Others(For your practice, solutions are available in Map_Reduce folder) - 

=> Count Friends by Age 

=> Temperature extremes (Minimum and Maximum) by Location

=> Word frequency counter

=> Word frequency counter using regular expressions

=> Sorting the frequency counter results using chained Map Reduce jobs

=> Customer orders count (based on individual customer)

=> Most popular movie

=> Most popular super hero

=> Similar movies (recommendation engine based on cosine similarity) 


# ***** Running map reduce jobs on Horton works virtual machine **** 

# First of all we need to install required softwares for Map reduce

# To install softwares on the Hortonworks sandbox we need to login as superuser by using command below
# su root 
(Initial password is hadoop and you are required to change it immediately)
Note: we changed the root password to "bigdata@roma" (For your future reference)

# If you have installated ***HDP 2.6.5*** (Latest sandbox version) -

# Install PIP -
  Utility for installing Python packages
  yum install python-pip
  
# Install MR job
  pip install mrjob==0.5.11
  
# Install Nano text editor
  yum install nano
  
# Download data files and script
  wget http://media.sundog-soft.com/hadoop/ml-100k/u.data
  wget http://media.sundog-soft.com/hadoop/RatingsBreakdown.py
  
# If you have installated ***HDP 2.5*** (Old sandbox version) -

# Install PIP -
  cd /etc/yum.repos.d
  cp sandbox.repo /tmp
  rm sandbox.repo (Ensure that you type yes or y after executing this command, otherwise the file will not be deleted)
  cd $HOME
  yum install python-pip
  
# Install MR job - 
  pip install google-api-python-client==1.6.4
  pip install mrjob==0.5.11
  
# Install Nano text editor
  yum install nano
   
# Download data files and script
  wget http://media.sundog-soft.com/hadoop/ml-100k/u.data - this is our ratings data file from our movie lens dataset
  wget http://media.sundog-soft.com/hadoop/RatingsBreakdown.py - this is the map reduce script 
  
  
 # Using nano (you can use vi editor as well)
   nano file_name.py  - to open a file using nano editor
   ctrl + x - to come out of nano editor
   ctrl + o - to edit a file
   
  # Running with mrjob
  
  # Run locally - 
     python RatingsBreakdown.py u.data
     
  # Run with Hadoop - 
     python RatingsBreakdown.py -r hadoop 
     --hadoop-streaming-jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar u.data
  
  # Run with Amazon EMR(Elastic map reduce) - 
  
  Note: For running a MapReduce(MR) job on Amazon EMR you first need an account on aws.amazon.com. 
        Once you give the card details, (you will be billed per usage and the cost of usage is also very less)
        you will be able to access the security keys (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
        to access map reduce on your local machine
  
  To run on a single EMR node: 
      > python MostPopularMovie.py -r emr --items=ml-100k/u.item   
          ml-100k/u.data
          
   To run on 4 EMR nodes: 
       > python MostPopularMovie.py -r emr --num-ec2-instances=4 --  
           items=ml-100k/u.item ml-100k/u.data

    Note: For the above commands to work we should be setting    
         AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY 
         prior to execution of the python script. 
         
         For windows: you need to create an environmental variables for these 2
         For Linux and Mac: you can just do export AWS_ACCESS_KEY_ID=$key_value$

 Other MR scripts:
 For sorting the results:
 # wget http://media.sundog-soft.com/hadoop/TopMovies.py 
 (download this file in the same user folder /user/maria_dev since u.data file is available in this folder.)
 
  Other notes: 
  Sort results in reducer: yield str(sum(values)).zfill(5), key
